{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from collections import Counter\n",
    "from typing import Tuple "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 처리 모듈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_handler.py\n",
    "# Vocabulary class to handle token-index mapping\n",
    "class Vocabulary:\n",
    "    \"\"\"\n",
    "    A class to handle the mapping between words and their corresponding indices. This class also handles special tokens\n",
    "    like PAD, SOS, EOS, and OOV which are essential in sequence modeling for padding, indicating start, end of sequences, \n",
    "    and out-of-vocabulary tokens.\n",
    "\n",
    "    Attributes:\n",
    "        PAD (str): Padding token.\n",
    "        SOS (str): Start-of-sequence token.\n",
    "        EOS (str): End-of-sequence token.\n",
    "        OOV (str): Out-of-vocabulary token.\n",
    "        pad_idx (int): Index of the padding token.\n",
    "        sos_idx (int): Index of the start-of-sequence token.\n",
    "        eos_idx (int): Index of the end-of-sequence token.\n",
    "        oov_idx (int): Index of the out-of-vocabulary token.\n",
    "        word2index (dict): Mapping of words to indices.\n",
    "        index2word (dict): Mapping of indices to words.\n",
    "        word_count (dict): Count of each word's occurrences.\n",
    "        n_words (int): Total number of words including special tokens.\n",
    "    \"\"\"\n",
    "    PAD = '[PAD]'\n",
    "    SOS = '[SOS]'\n",
    "    EOS = '[EOS]'\n",
    "    OOV = '[OOV]'\n",
    "    pad_idx = 0\n",
    "    sos_idx = 1\n",
    "    eos_idx = 2\n",
    "    oov_idx = 3\n",
    "    SPECIAL_TOKENS = [PAD, SOS, EOS, OOV]\n",
    "\n",
    "    def __init__(self, word_count_threshold = 0):\n",
    "        \"\"\"\n",
    "        Initializes the Vocabulary class, adding special tokens and initializing the word-index mappings.\n",
    "        \n",
    "        Args:\n",
    "            word_count_threshold (float): The threshold for filtering OOV words. \n",
    "        \"\"\"\n",
    "        self.word2index = {}\n",
    "        self.index2word = {}\n",
    "        self.word_count = {}\n",
    "        self.n_words = 0  # Count PAD, SOS, EOS, OOV tokens\n",
    "        self.threshold = word_count_threshold\n",
    "\n",
    "        # Special tokens\n",
    "        self.pad_idx = Vocabulary.pad_idx\n",
    "        self.sos_idx = Vocabulary.sos_idx\n",
    "        self.eos_idx = Vocabulary.eos_idx\n",
    "        self.oov_idx = Vocabulary.oov_idx\n",
    "\n",
    "        # Initialize the special tokens\n",
    "        self.add_word(Vocabulary.PAD)\n",
    "        self.add_word(Vocabulary.SOS)\n",
    "        self.add_word(Vocabulary.EOS)\n",
    "        self.add_word(Vocabulary.OOV)\n",
    "\n",
    "    def add_word(self, word):\n",
    "        \"\"\"\n",
    "        Adds a word to the vocabulary if it doesn't exist, or increments its count if it does.\n",
    "\n",
    "        Args:\n",
    "            word (str): The word to add.\n",
    "        \"\"\"\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word_count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words +=  1\n",
    "        else:\n",
    "            self.word_count[word] +=  1\n",
    "\n",
    "    def add_sentence(self, sentence):\n",
    "        \"\"\"\n",
    "        Adds each word in a sentence to the vocabulary.\n",
    "\n",
    "        Args:\n",
    "            sentence (list of str): List of words to add.\n",
    "        \"\"\"\n",
    "        for word in sentence:\n",
    "            self.add_word(word)\n",
    "\n",
    "    def word_to_index(self, word):\n",
    "        \"\"\"\n",
    "        Maps a word to its corresponding index. If the word is not found, it returns the index for OOV.\n",
    "\n",
    "        Args:\n",
    "            word (str): The word to convert.\n",
    "\n",
    "        Returns:\n",
    "            int: Index of the word, or the OOV index if the word is not in the vocabulary.\n",
    "        \"\"\"\n",
    "        return self.word2index.get(word, self.oov_idx)\n",
    "\n",
    "def indices_to_one_hot(batch, vocab_size):\n",
    "    \"\"\"\n",
    "    Converts a batch of indices to one-hot encoded vectors.\n",
    "\n",
    "    Args:\n",
    "        batch (torch.Tensor): Input tensor of indices with shape [batch_size, seq_length].\n",
    "        vocab_size (int): Size of the vocabulary for the one-hot encoding.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: One-hot encoded tensor with shape [batch_size, seq_length, vocab_size].\n",
    "    \"\"\"\n",
    "    assert batch.dim() == 2, f\"Input batch should have 2 dimensions, but got {batch.dim()}\"\n",
    "    batch_size, seq_length = batch.size()\n",
    "    \n",
    "    one_hot = torch.zeros(batch_size, seq_length, vocab_size).to(batch.device)\n",
    "    one_hot.scatter_(2, batch.unsqueeze(2), 1)  # [batch_size, seq_length, vocab_size]\n",
    "    \n",
    "    assert one_hot.size() == (batch_size, seq_length, vocab_size), f\"Output should have shape {(batch_size, seq_length, vocab_size)}, but got {one_hot.size()}\"\n",
    "    \n",
    "    return one_hot\n",
    "\n",
    "def generate_target_sequence(input_seq, length, lookback = 3, mod = 10):\n",
    "    res = []\n",
    "    \n",
    "    for idx in range(length):\n",
    "        if idx < lookback:\n",
    "            res.append(sum(input_seq[-lookback+idx:] + res[:idx]) % mod)\n",
    "        else:\n",
    "            res.append(sum(res[-lookback:]) % mod)\n",
    "    \n",
    "    return res \n",
    "\n",
    "def create_integer_sequence_dataset(num_samples = 1000, max_input_len = 10, max_target_len = 15, vocab_size = 20, batch_size = 32, train_valid_test = (0.8, 0.1, 0.1)):\n",
    "    \"\"\"\n",
    "    Creates a dataset of integer sequences, where input sequences are randomly generated and their targets are sorted sequences.\n",
    "\n",
    "    Args:\n",
    "        num_samples (int): The number of samples to generate in the dataset.\n",
    "        max_input_len (int): The maximum length of input sequences in the dataset.\n",
    "        max_target_len (int): The maximum length of target sequences in the dataset.\n",
    "        vocab_size (int): The size of the vocabulary.\n",
    "        batch_size (int): The batch size for the DataLoader.\n",
    "        train_valid_test (tuple): Proportions for splitting the dataset into training, validation, and testing sets.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the list of DataLoader objects for training, validation, and testing sets, \n",
    "               and the associated Vocabulary object.\n",
    "    \"\"\"\n",
    "    vocab = Vocabulary()\n",
    "    no_special_token = len(Vocabulary.SPECIAL_TOKENS)\n",
    "    for i in range(no_special_token, vocab_size):  \n",
    "        vocab.add_word(str(i))\n",
    "\n",
    "    data = []\n",
    "    for _ in range(num_samples):\n",
    "        input_len = random.randint(3, max_input_len)\n",
    "        \n",
    "        input_seq = [random.randint(no_special_token, vocab_size - 1) for _ in range(input_len)]\n",
    "        target_seq = input_seq \n",
    "        # target_seq = generate_target_sequence(input_seq, max_target_len)\n",
    "        \n",
    "        # input_seq = [(no_special_token + i) % vocab_size for i in range(input_len)]\n",
    "        # target_seq = [(input_seq[-1] + i) % vocab_size for i in range(len(input_seq))]\n",
    "\n",
    "        data.append((input_seq, target_seq))\n",
    "\n",
    "    # Split dataset\n",
    "    lengths = [int(num_samples * ratio) for ratio in train_valid_test]\n",
    "    lengths[-1] = num_samples - sum(lengths[:-1])  # Adjust the last set size to account for rounding\n",
    "\n",
    "    datasets = random_split(data, lengths)\n",
    "    dataloaders = [DataLoader(dataset, batch_size = batch_size, shuffle = True, collate_fn = collate_fn) for dataset in datasets]\n",
    "\n",
    "    return dataloaders, vocab\n",
    "\n",
    "\n",
    "def create_lang_pair(data_file, batch_size = 32, train_valid_test = (0.8, 0.1, 0.1)):\n",
    "    source_vocab = Vocabulary()\n",
    "    target_vocab = Vocabulary()\n",
    "\n",
    "    text = open(data_file, 'r', encoding = 'utf-8').read() \n",
    "    data = []\n",
    "    num_samples = 0\n",
    "    for line in text.split('\\n'):\n",
    "        line = line.strip()\n",
    "        try:\n",
    "            source, target, _ = line.split('\\t')\n",
    "        except ValueError:\n",
    "            try:\n",
    "                source, target = line.split('\\t') \n",
    "            except ValueError:\n",
    "                continue \n",
    "\n",
    "        source = source.strip().split()\n",
    "        target = target.strip().split()\n",
    "        source_vocab.add_sentence(source)\n",
    "        target_vocab.add_sentence(target)\n",
    "\n",
    "        data.append((source, target))\n",
    "        num_samples += 1\n",
    "\n",
    "    lengths = [int(num_samples * ratio) for ratio in train_valid_test]\n",
    "    lengths[-1] = num_samples - sum(lengths[:-1])  # Adjust the last set size to account for rounding\n",
    "\n",
    "    datasets = random_split(data, lengths)\n",
    "    dataloaders = [DataLoader(dataset, batch_size = batch_size, shuffle = True, collate_fn = lambda x: collate_fn_language(x, source_vocab, target_vocab)) for dataset in datasets]\n",
    "\n",
    "    return dataloaders, source_vocab, target_vocab\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function for batching integer sequence data.\n",
    "    \n",
    "    Args:\n",
    "        batch (list of tuple): List of input-output sequence pairs where input is an unsorted sequence of integers, and target is the sorted sequence.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Padded input and target tensors of shape [batch_size, max_seq_len].\n",
    "    \"\"\"\n",
    "    input_seqs = [item[0] for item in batch]\n",
    "    target_seqs = [item[1] for item in batch]\n",
    "\n",
    "    # Add EOS token to sequences\n",
    "    input_seqs = [seq + [Vocabulary.eos_idx] for seq in input_seqs]\n",
    "    target_seqs = [[Vocabulary.sos_idx] + seq + [Vocabulary.eos_idx] for seq in target_seqs]\n",
    "\n",
    "    # Get max sequence lengths\n",
    "    input_max_len = max([len(seq) for seq in input_seqs])\n",
    "    target_max_len = max([len(seq) for seq in target_seqs])\n",
    "\n",
    "    # Pad sequences\n",
    "    input_padded = []\n",
    "    for seq in input_seqs:\n",
    "        seq = seq + [Vocabulary.pad_idx] * (input_max_len - len(seq))\n",
    "        input_padded.append(seq)\n",
    "\n",
    "    target_padded = []\n",
    "    for seq in target_seqs:\n",
    "        seq = seq + [Vocabulary.pad_idx] * (target_max_len - len(seq))\n",
    "        target_padded.append(seq)\n",
    "\n",
    "    # Convert to tensors\n",
    "    input_padded = torch.tensor(input_padded, dtype = torch.long)  # [batch_size, max_seq_len]\n",
    "    target_padded = torch.tensor(target_padded, dtype = torch.long)  # [batch_size, max_seq_len]\n",
    "\n",
    "    return input_padded, target_padded\n",
    "\n",
    "def collate_fn_language(batch, source_vocab, target_vocab):\n",
    "    \"\"\"\n",
    "    Custom collate function for batching language sequence data.\n",
    "\n",
    "    Args:\n",
    "        batch (list of tuple): List of input-output sequence pairs where input is an unsorted sequence of words, \n",
    "                               and target is the reversed sequence.\n",
    "        vocab (Vocabulary): Vocabulary object for word-to-index conversions.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Padded input and target tensors of shape [batch_size, max_seq_len].\n",
    "    \"\"\"\n",
    "    input_seqs = [item[0] for item in batch]\n",
    "    target_seqs = [item[1] for item in batch]\n",
    "\n",
    "    # Convert words to indices and add EOS token\n",
    "    input_seqs_indices = []\n",
    "    for seq in input_seqs:\n",
    "        seq_indices = [source_vocab.word_to_index(word) for word in seq] + [source_vocab.eos_idx]\n",
    "        input_seqs_indices.append(seq_indices)\n",
    "\n",
    "    target_seqs_indices = []\n",
    "    for seq in target_seqs:\n",
    "        seq_indices = [target_vocab.sos_idx] + [target_vocab.word_to_index(word) for word in seq] + [target_vocab.eos_idx]\n",
    "        target_seqs_indices.append(seq_indices)\n",
    "\n",
    "    # Get max sequence lengths\n",
    "    input_max_len = max([len(seq) for seq in input_seqs_indices])\n",
    "    target_max_len = max([len(seq) for seq in target_seqs_indices])\n",
    "\n",
    "    # Pad sequences\n",
    "    input_padded = []\n",
    "    for seq in input_seqs_indices:\n",
    "        seq = seq + [source_vocab.pad_idx] * (input_max_len - len(seq))\n",
    "        input_padded.append(seq)\n",
    "\n",
    "    target_padded = []\n",
    "    for seq in target_seqs_indices:\n",
    "        seq = seq + [target_vocab.pad_idx] * (target_max_len - len(seq))\n",
    "        target_padded.append(seq)\n",
    "\n",
    "    # Convert to tensors\n",
    "    input_padded = torch.tensor(input_padded, dtype = torch.long)  # [batch_size, max_seq_len]\n",
    "    target_padded = torch.tensor(target_padded, dtype = torch.long)  # [batch_size, max_seq_len]\n",
    "\n",
    "    return input_padded, target_padded\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 데이터셋을 생성하는 함수 호출\n",
    "    # 'kor.txt' 파일을 읽어 (train, valid, test) 데이터와 source_vocab, target_vocab을 생성\n",
    "    (train_data, valid_data, test_data), source_vocab, target_vocab = create_lang_pair('kor.txt')\n",
    "\n",
    "    # 학습 데이터에서 첫 번째 샘플을 출력하는 루프\n",
    "    for input_sequence, target_sequence in train_data:\n",
    "        # 첫 번째 입력 시퀀스 출력\n",
    "        print(\"첫 번째 입력 시퀀스 (Input Sequence):\", input_sequence[0].tolist())  # 첫 번째 입력 시퀀스의 값을 리스트로 변환하여 출력\n",
    "        print(\"첫 번째 타겟 시퀀스 (Target Sequence):\", target_sequence[0].tolist())  # 첫 번째 타겟 시퀀스의 값을 리스트로 변환하여 출력\n",
    "        \n",
    "        # 각 시퀀스의 의미를 설명\n",
    "        print(\"\\n# 각 시퀀스의 의미\")\n",
    "        print(\"- 입력 시퀀스는 모델에 주어진 단어의 인덱스를 나타냅니다.\")\n",
    "        print(\"- 타겟 시퀀스는 모델이 예측해야 하는 정답 단어의 인덱스를 나타냅니다.\")\n",
    "        \n",
    "        # 더 많은 정보를 출력하기 위해 각 시퀀스의 길이도 출력\n",
    "        print(f\"\\n입력 시퀀스의 길이: {len(input_sequence[0])} (단어 수)\")\n",
    "        print(f\"타겟 시퀀스의 길이: {len(target_sequence[0])} (단어 수)\")\n",
    "        \n",
    "        break  # 첫 번째 샘플만 출력하고 루프 종료\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 이하\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- seq2seq\n",
    "- 인코더\n",
    "  - 입력문장을 읽고 인코딩\n",
    "  - RNN구조로 입력 문장의 단어를 순차적 처리\n",
    "  - 처리 후 최종 hidden state를 문장 임베딩으로 사용\n",
    "- 디코더\n",
    "  - 인코딩의 문장 임베딩을 입력으로 받아 디코딩하여 한 단어씩 출력, 번역된 문장을 생성\n",
    "- 번역과정\n",
    "  1. 인코더가 입력문장을 단어 단위로 읽어 문장 임베딩을 생성\n",
    "  2. 디코더는 문장 임베딩과 이전에 생성된 단어를 입력으로 받아 다음 단어의 확률을 계산\n",
    "  3. 가장 높은 확률의 단어를 선택하여 출력\n",
    "  4. 2-3 과정을 반복 전체 번역문 생성\n",
    "  5. 특수 토큰(ex:<EOS>)이 생성되면 번역 종료\n",
    "- Beam Search: 첫 단어 선택의 오류를 방지하기 위해 Top-K개의 가능성 있는 번역 후보를 유지하며 번역을 진행\n",
    "- Teacher Forcing: 학습 시 이전 시점의 실제 정답 단어를 다음 입력으로 사용하여 학습 속도와 성능을 향상\n",
    "- Attention 메커니즘: 긴 문장 번역 시 성능 향상을 위해 입력 문장의 관련 부분에 집중하는 기법을 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(seq_len, n_samples, vocab_size):\n",
    "    # torch.randint(최소값, 최대값, 사이즈): 1~vocab_size-1 범위의 무작위 정수로 구성된 텐서\n",
    "    inputs = torch.randint(1, vocab_size, (n_samples, seq_len))\n",
    "    outputs = inputs.clone() # 복사본 대입\n",
    "    return TensorDataset(inputs, outputs)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        hidden_size: int\n",
    "        ):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.linear = nn.Linear(input_size, hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "    \n",
    "    def forward(self, input_seq: torch.tensor) -> torch.tensor:\n",
    "        batch_size, seq_length = input_seq.size()\n",
    "        hidden = torch.zeros(batch_size, self.hidden_size) # 은닉상태값 초기화\n",
    "        # 시퀀스 길이로 각 문자 인덱스 반복\n",
    "        for char_idx in range(seq_length): # 각 문자 인덱스를 원핫 인코딩으로변환\n",
    "            x_t = nn.functional.one_hot(input_seq[:, char_idx],\n",
    "                                        num_classes = self.linear.in_features).float()\n",
    "            hidden = self.activation(self.linear(x_t) + hidden)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def generate_dataset(seq_length, num_sample, vocab_size):\n",
    "    inputs = torch.randint(1, vocab_size, (num_sample, seq_length))\n",
    "    outputs = inputs.clone()\n",
    "\n",
    "    return TensorDataset(inputs, outputs)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.linear = nn.Linear(input_size, hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        batch_size, seq_length = input_seq.size() # batch_size, seq_elngth\n",
    "        hidden = torch.zeros(batch_size, self.hidden_size).to(device)\n",
    "\n",
    "        for char_idx in range(seq_length):\n",
    "            x_t = nn.functional.one_hot(input_seq[:, char_idx], num_classes = self.linear.in_features).float()\n",
    "            hidden = self.activation(self.linear(x_t) + hidden)\n",
    "\n",
    "        return hidden\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # self.i2h = nn.Linear(input_size, hidden_size) # input -> hidden\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "        self.linear2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, target_seq, hidden):\n",
    "        batch_size, seq_len = target_seq.size()\n",
    "        outputs = torch.zeros(batch_size, seq_len, self.output_size).to(device)\n",
    "\n",
    "        for char_idx in range(seq_len):\n",
    "            if char_idx == 0:\n",
    "                previous_y = torch.zeros(batch_size, self.input_size).to(device)\n",
    "            else:\n",
    "                y_prev = target_seq[:, char_idx - 1]\n",
    "                previous_y = nn.functional.one_hot(y_prev, self.input_size).to(device).float()\n",
    "            hidden = self.activation(self.linear1(previous_y) + hidden)\n",
    "            output = self.linear2(hidden)\n",
    "\n",
    "            outputs[:, char_idx, :] = output\n",
    "\n",
    "        return outputs\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, input_seq, target_seq):\n",
    "        encoder_hidden = self.encoder(input_seq)\n",
    "        decoder_output = self.decoder(target_seq, encoder_hidden)\n",
    "\n",
    "        return decoder_output\n",
    "\n",
    "def train_model(model, dataloader, criterion, optimizer, num_epochs, device):\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for inputs, targets in dataloader:\n",
    "            # inputs.shape - batch_size, sequence_length\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs, targets)\n",
    "            outputs = outputs.view(-1, outputs.size(-1)) # batch_size * seq_length, output_size\n",
    "            targets = targets.view(-1) # batch_size * seq_len\n",
    "\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        print(f'Epoch {epoch}, loss: {avg_loss}')\n",
    "\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            outputs = model(inputs, targets) # batch_size, seq_length, vocab_size\n",
    "\n",
    "            predicted = torch.argmax(outputs, dim = 2)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "            total += targets.size(0) * targets.size(1)\n",
    "    acc = correct / total\n",
    "    return acc\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    seq_length = 10\n",
    "    num_samples = 1000\n",
    "    vocab_size = 5  # Including a padding index if needed\n",
    "    hidden_size = 64\n",
    "    batch_size = 32\n",
    "    num_epochs = 20\n",
    "    learning_rate = 0.001\n",
    "\n",
    "    # Device configuration\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # print(f\"Using device: {device}\")\n",
    "\n",
    "    dataset = generate_dataset(seq_length, num_samples, vocab_size)\n",
    "    dataloader = DataLoader(dataset, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "    encoder = Encoder(input_size = vocab_size, hidden_size = hidden_size)\n",
    "    decoder = Decoder(input_size = vocab_size, hidden_size = hidden_size, output_size = vocab_size)\n",
    "\n",
    "    model = Seq2Seq(encoder, decoder).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "    train_model(model, dataloader, criterion, optimizer, num_epochs, device)\n",
    "\n",
    "    acc = evaluate_model(model, dataloader, device)\n",
    "    print(f\"Training Accuracy: {acc * 100:.2f}%\\n\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        test_input, test_target = dataset[0]\n",
    "        test_input = test_input.unsqueeze(0).to(device)\n",
    "        test_target = test_target.unsqueeze(0).to(device)\n",
    "\n",
    "        output = model(test_input, test_target)\n",
    "\n",
    "        predicted = torch.argmax(output, dim = 2)\n",
    "        print(\"Sample Input Sequence:   \", test_input.squeeze().tolist())\n",
    "        print(\"Sample Target Sequence:  \", test_target.squeeze().tolist())\n",
    "        print(\"Predicted Sequence       :  \", predicted.squeeze().tolist())\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
